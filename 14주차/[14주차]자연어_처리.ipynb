{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "자연어(NLP)"
      ],
      "metadata": {
        "id": "8nLlO_1caxjM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 관련 NLP패키지를 설치\n",
        "- 텍스트 데이터를 토큰화\n",
        "- 형태소 분석을 통해 단어를 기본 형식으로 변환\n",
        "- 텍스트 데이터를 청크로 나누기\n",
        "- word beg모델을 사용해서 문서-용어 행렬 추출\n",
        "- 카테고리 예측기\n",
        "- 성별 식별자 구축\n",
        "- 감정 분석기 구축\n",
        "- 모델링"
      ],
      "metadata": {
        "id": "0MmeXyXCbEgA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK (Natural Language Tookit)\n",
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63rLr3bbbvxF",
        "outputId": "58421476-325b-462a-8a0a-dca0327f655e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 시멘틱 모델링에 사용\n",
        "# 시멘틱 : 텍스트 데이터의 의미와 관련된 정보를 모델링하는 작업\n",
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWspaEy3cohS",
        "outputId": "183181b1-bfa3-4f8e-c439-4df346eb4da8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.2)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pattern"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-Y6sYUidrao",
        "outputId": "4bab7b2e-2e89-4e06-d409-93c68e121ab4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pattern\n",
            "  Downloading Pattern-3.6.0.tar.gz (22.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pattern) (0.18.3)\n",
            "Collecting backports.csv (from pattern)\n",
            "  Downloading backports.csv-1.0.7-py2.py3-none-any.whl (12 kB)\n",
            "Collecting mysqlclient (from pattern)\n",
            "  Downloading mysqlclient-2.2.0.tar.gz (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.5/89.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from pattern) (4.11.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from pattern) (4.9.3)\n",
            "Collecting feedparser (from pattern)\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six (from pattern)\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pattern) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pattern) (1.11.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from pattern) (3.8.1)\n",
            "Collecting python-docx (from pattern)\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cherrypy (from pattern)\n",
            "  Downloading CherryPy-18.8.0-py2.py3-none-any.whl (348 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.4/348.4 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pattern) (2.31.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->pattern) (2.5)\n",
            "Collecting cheroot>=8.2.1 (from cherrypy->pattern)\n",
            "  Downloading cheroot-10.0.0-py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portend>=2.1.1 (from cherrypy->pattern)\n",
            "  Downloading portend-3.2.0-py3-none-any.whl (5.3 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from cherrypy->pattern) (10.1.0)\n",
            "Collecting zc.lockfile (from cherrypy->pattern)\n",
            "  Downloading zc.lockfile-3.0.post1-py3-none-any.whl (9.8 kB)\n",
            "Collecting jaraco.collections (from cherrypy->pattern)\n",
            "  Downloading jaraco.collections-4.3.0-py3-none-any.whl (11 kB)\n",
            "Collecting sgmllib3k (from feedparser->pattern)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->pattern) (4.66.1)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->pattern) (3.2.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->pattern) (41.0.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pattern) (2023.7.22)\n",
            "Collecting jaraco.functools (from cheroot>=8.2.1->cherrypy->pattern)\n",
            "  Downloading jaraco.functools-3.9.0-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->pattern) (1.15.1)\n",
            "Collecting tempora>=1.8 (from portend>=2.1.1->cherrypy->pattern)\n",
            "  Downloading tempora-5.5.0-py3-none-any.whl (13 kB)\n",
            "Collecting jaraco.text (from jaraco.collections->cherrypy->pattern)\n",
            "  Downloading jaraco.text-3.11.1-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zc.lockfile->cherrypy->pattern) (67.7.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->pattern) (2.21)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern) (2023.3.post1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from jaraco.functools->cheroot>=8.2.1->cherrypy->pattern) (4.5.0)\n",
            "Collecting jaraco.context>=4.1 (from jaraco.text->jaraco.collections->cherrypy->pattern)\n",
            "  Downloading jaraco.context-4.3.0-py3-none-any.whl (5.3 kB)\n",
            "Collecting autocommand (from jaraco.text->jaraco.collections->cherrypy->pattern)\n",
            "  Downloading autocommand-2.2.2-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (7.0.0)\n",
            "Requirement already satisfied: pydantic>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (1.10.12)\n",
            "Building wheels for collected packages: pattern, mysqlclient, python-docx, sgmllib3k\n",
            "  Building wheel for pattern (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pattern: filename=Pattern-3.6-py3-none-any.whl size=22332702 sha256=fdcd569025659f3974a7025052f2eb7f6538a35e433f4af7df142f814f257099\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/8f/40/fe23abd593ef60be5bfaf3e02154d3484df42aa947bbf4d499\n",
            "  Building wheel for mysqlclient (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mysqlclient: filename=mysqlclient-2.2.0-cp310-cp310-linux_x86_64.whl size=123663 sha256=3a3e381cb97d1cc7587926a227dcba586a2766d038521500c3319f2536fb1327\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/f8/fd/0399687c0abd03c10c975ed56c692fcd3d0fb80440b5a661f1\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184487 sha256=430c61ed09ef6588c687f859843de5d2f1b03fc8d3c62d23dac1e1e45e75868d\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/27/06/837436d4c3bd989b957a91679966f207bfd71d358d63a8194d\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=cf30ca9f73c776c70c7fbf6ea8c85cf23b043a2347bdd9cf918ed662df3e81b4\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built pattern mysqlclient python-docx sgmllib3k\n",
            "Installing collected packages: sgmllib3k, backports.csv, zc.lockfile, python-docx, mysqlclient, jaraco.functools, jaraco.context, feedparser, autocommand, tempora, cheroot, portend, pdfminer.six, jaraco.text, jaraco.collections, cherrypy, pattern\n",
            "Successfully installed autocommand-2.2.2 backports.csv-1.0.7 cheroot-10.0.0 cherrypy-18.8.0 feedparser-6.0.10 jaraco.collections-4.3.0 jaraco.context-4.3.0 jaraco.functools-3.9.0 jaraco.text-3.11.1 mysqlclient-2.2.0 pattern-3.6 pdfminer.six-20221105 portend-3.2.0 python-docx-0.8.11 sgmllib3k-1.0.0 tempora-5.5.0 zc.lockfile-3.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 텍스트데이터 토근화"
      ],
      "metadata": {
        "id": "6VWth-2ceBAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOLvmXJyepj0",
        "outputId": "0fa4d21b-337b-4c07-bc60-6ab0fa209edd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize, WordPunctTokenizer"
      ],
      "metadata": {
        "id": "g8wPOP2Fd7lS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"do you know how tokenize works? \\\n",
        "It's actually quite interesting!\"\n",
        "\n",
        "# 문장 토큰화\n",
        "sent_tokenize(input_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuFeITdNeN_5",
        "outputId": "7abc92ac-6c09-4407-b1b5-db614652a1b5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['do you know how tokenize works?', \"It's actually quite interesting!\"]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 토큰화\n",
        "print(\"Word tokenizer\")\n",
        "print(word_tokenize(input_text))"
      ],
      "metadata": {
        "id": "Qro9rNKGegjQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6402b23-c266-46b1-88bd-0c333ac4d228"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word tokenizer\n",
            "['do', 'you', 'know', 'how', 'tokenize', 'works', '?', 'It', \"'s\", 'actually', 'quite', 'interesting', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WordPunctTokenizer를 사용해서 입력 텍스트를 단어 토큰으로 분할\n",
        "print(\"WordPunctTokenizer\")\n",
        "print(WordPunctTokenizer().tokenize(input_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgncl-sFEvXi",
        "outputId": "01807521-be41-4854-bab1-5757f3056186"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WordPunctTokenizer\n",
            "['do', 'you', 'know', 'how', 'tokenize', 'works', '?', 'It', \"'\", 's', 'actually', 'quite', 'interesting', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "형태소 분석을 사용해서 단어를 기본 형식으로 변환\n",
        "  - sing\n",
        "    - singer, singing, song, sung 등 다양하게 사용\n",
        "    - 의미가 유사하며, 이러한 과정을 형태소 분석\n",
        "    - 어간/기본간어의 형택학적 변형을 생성하는 방법"
      ],
      "metadata": {
        "id": "llrf2S6qGL5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 형태소 분석기\n",
        "# 다양한 형태의 단어를 줄이는게 목표\n",
        "# 단어의 끝을 잘라내서 기본형태를 추출하는 프로세스 - 휴리스틱 프로세스"
      ],
      "metadata": {
        "id": "N7tPpY84FL3h"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer"
      ],
      "metadata": {
        "id": "JByi0gvFG-H9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력단어 정의\n",
        "input_word = [\n",
        "    'writing', 'calves','be','branded','horse','radomize','possibly','provision','hospital','kept','scratchy','code'\n",
        "]"
      ],
      "metadata": {
        "id": "w0Ezcg46HOyR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 형태소 분석기들을 생성\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "snowball = SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "Yx0MYVe9Hp_E"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어를 순환하면서 형태소를 추출하고 결과 표시\n",
        "print(f\"{'input_word':20s}{'porter':20s}{'lancaster':20s}{'snowball':20s}\")\n",
        "print(f\"=\"*80)\n",
        "for word in input_word:\n",
        "  print(f\"{word:20s}{porter.stem(word):20s}{lancaster.stem(word):20s}{snowball.stem(word):20s}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20LF7ke-IHda",
        "outputId": "c24e412f-3b53-406a-ac1c-2ec8ab643446"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_word          porter              lancaster           snowball            \n",
            "================================================================================\n",
            "writing             write               writ                write               \n",
            "calves              calv                calv                calv                \n",
            "be                  be                  be                  be                  \n",
            "branded             brand               brand               brand               \n",
            "horse               hors                hors                hors                \n",
            "radomize            radom               radom               radom               \n",
            "possibly            possibl             poss                possibl             \n",
            "provision           provis              provid              provis              \n",
            "hospital            hospit              hospit              hospit              \n",
            "kept                kept                kept                kept                \n",
            "scratchy            scratchi            scratchy            scratchi            \n",
            "code                code                cod                 code                \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "단어를 기본형식으로 변화 - 기본형화(Lemmatizer)\n",
        "  - 형태소 분석기를 통한 단어 표준화를 하면 의미 없는 단어가 생성되기도 함\n",
        "  - 형태소 분석기와 비슷하지만 단어에 맥락을 제공 그래서 의미가 비슷한 단어를 한 단어로 연결"
      ],
      "metadata": {
        "id": "QwNUK5R6KpN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "YnY76yE6IrtE"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "SQA6pqJFLV6N"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iU4-n8nzLz9z",
        "outputId": "a21b2e4d-d440-4528-f8fe-1c6dc0b7073c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{'input_word':20s}{'명사':20s}{'동사':20s}\")\n",
        "for word in input_word:\n",
        "  print(f\"{word:20s}{lemmatizer.lemmatize(word,pos='n'):20s}{lemmatizer.lemmatize(word,pos='v'):20s}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eIF9zPdLdnR",
        "outputId": "6452b55e-51d1-41c6-be85-f10e3a08c41a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_word          명사                  동사                  \n",
            "writing             writing             write               \n",
            "calves              calf                calve               \n",
            "be                  be                  be                  \n",
            "branded             branded             brand               \n",
            "horse               horse               horse               \n",
            "radomize            radomize            radomize            \n",
            "possibly            possibly            possibly            \n",
            "provision           provision           provision           \n",
            "hospital            hospital            hospital            \n",
            "kept                kept                keep                \n",
            "scratchy            scratchy            scratchy            \n",
            "code                code                code                \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "텍스트 데이터를 청크로 나누기\n",
        "  - 텍스트데이터를 추가 분석을 위해서 여러 조각으로 나눈다 - 청킹(chunking)\n",
        "  - 청킹은 토큰화와 다르다\n",
        "  - 큰 텍스트는 문서를 다룰때 의미있는 정보를 추출하기위해서 텍스트를 청크로 분활"
      ],
      "metadata": {
        "id": "sdy2lMguMh70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from nltk.corpus import brown"
      ],
      "metadata": {
        "id": "vr9JvXhbLvpB"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 텍스트를 청크로 나누는 함수를 정의\n",
        "# 각 청크는 단어 N개를 포함한다\n",
        "def chunker(input_data,N):\n",
        "  input_words =  input_data.split()\n",
        "  output = []\n",
        "  cur_chunk = []\n",
        "  count = 0\n",
        "  # 단어들을 순환하면서 청크로 분할\n",
        "  for word in input_words:\n",
        "    cur_chunk.append(word)\n",
        "    count +=1\n",
        "    if count == N:\n",
        "      output.append(' '.join(cur_chunk))\n",
        "      count,cur_chunk = 0, []\n",
        "\n",
        "  output.append(' '.join(cur_chunk))\n",
        "  return output"
      ],
      "metadata": {
        "id": "88YSDEHgNAVJ"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 브라운 말뭉치를 사용해서 입력데이터를 읽어온다\n",
        "import nltk\n",
        "nltk.download('brown')"
      ],
      "metadata": {
        "id": "P__iC2Z0S_9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_data = ' '.join(brown.words()[:12000])\n",
        "# 각 청크에 있는 단어 개수를 정의\n",
        "chunk_size = 700\n",
        "# 입력데이터를 청크로 분할하고 결과를 표시\n",
        "chunks = chunker(input_data,chunk_size)\n",
        "print(f\"number of chunks : {len(chunks)}\")\n",
        "for i, chunk in enumerate(chunks):\n",
        "  print(f\"chunk {i+1} ==> {chunk[:50]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKp4GrEDQ1YS",
        "outputId": "6ca2a9e7-777c-4ce1-925b-36bf39caf4fe"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of chunks : 18\n",
            "chunk 1 ==> The Fulton County Grand Jury said Friday an invest\n",
            "chunk 2 ==> '' . ( 2 ) Fulton legislators `` work with city of\n",
            "chunk 3 ==> . Construction bonds Meanwhile , it was learned th\n",
            "chunk 4 ==> , anonymous midnight phone calls and veiled threat\n",
            "chunk 5 ==> Harris , Bexar , Tarrant and El Paso would be $451\n",
            "chunk 6 ==> set it for public hearing on Feb. 22 . The proposa\n",
            "chunk 7 ==> College . He has served as a border patrolman and \n",
            "chunk 8 ==> of his staff were doing on the address involved co\n",
            "chunk 9 ==> plan alone would boost the base to $5,000 a year a\n",
            "chunk 10 ==> nursing homes In the area of `` community health s\n",
            "chunk 11 ==> of its Angola policy prove harsh , there has been \n",
            "chunk 12 ==> system which will prevent Laos from being used as \n",
            "chunk 13 ==> reform in recipient nations . In Laos , the admini\n",
            "chunk 14 ==> . He is not interested in being named a full-time \n",
            "chunk 15 ==> said , `` to obtain the views of the general publi\n",
            "chunk 16 ==> '' . Mr. Reama , far from really being retired , i\n",
            "chunk 17 ==> making enforcement of minor offenses more effectiv\n",
            "chunk 18 ==> to tell the people where he stands on the tax issu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bag of Word model\n",
        "  - 텍스트의 빈도 추출\n",
        "  -  문서내의 모든 단어에서 어휘를 추출하고 document-term model 사용해서 모델을 구축\n",
        "  - 문서-용어 모델\n",
        "    - 단어개수만 추출하고 어순은 무시"
      ],
      "metadata": {
        "id": "AzU1WjgRTYW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. The children are playing in the hall\n",
        "# 2. The hall has a lot of space\n",
        "# 3. Lots of children like playing in an open space\n",
        "# 14개의 고유한 단어\n",
        "# 벡테로 표시\n",
        "# 1  [1,2,3,4,5,6,7,0,0,0,0,0,0   ]\n",
        "# 2  [1,7,8....          ]"
      ],
      "metadata": {
        "id": "WkKz0f9KRLJb"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "K3I2kmmqUl6S"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 브라운 말뭉치  5400개만\n",
        "input_data = ' '.join(brown.words()[:5400])\n",
        "chunk_size = 800\n",
        "text_chuncks = chunker(input_data, chunk_size)"
      ],
      "metadata": {
        "id": "f9-M5fQ8Uy56"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 청크를 dict 로 변경\n",
        "chunks = []\n",
        "for count, chunk in enumerate(text_chuncks):\n",
        "  d = {'index':count, 'text':chunk}\n",
        "  chunks.append(d)"
      ],
      "metadata": {
        "id": "-r_8ABEFVFgM"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 단어의 개수를 구하는 문서-용어 행렬 추출\n",
        "# 최소 문서빈도, 최대문서 빈도\n",
        "count_vectorizer = CountVectorizer(min_df=7,max_df=20)\n",
        "document_term_matrix =  count_vectorizer.fit_transform([  chunk['text']    for chunk in chunks])"
      ],
      "metadata": {
        "id": "BBU6eVUlVHAC"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 어휘를 추출하고 표시\n",
        "vocabulary =  count_vectorizer.get_feature_names_out()\n",
        "print(f\"vocabulary : {vocabulary}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbC3-atjWMfz",
        "outputId": "fa1166d9-a2e1-4757-aa21-2120fb0cb6c6"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary : ['and' 'are' 'be' 'by' 'county' 'for' 'in' 'is' 'it' 'of' 'on' 'one'\n",
            " 'said' 'state' 'that' 'the' 'to' 'two' 'was' 'which' 'with']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 청크이름을 생성\n",
        "chunk_names = []\n",
        "for i in range(len(text_chuncks)):\n",
        "  chunk_names.append(f'Chunk-{i+1}')\n",
        "chunk_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__SX0JaOWQfj",
        "outputId": "14ee13b6-d8e2-4bee-ac39-807014acd7ef"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Chunk-1', 'Chunk-2', 'Chunk-3', 'Chunk-4', 'Chunk-5', 'Chunk-6', 'Chunk-7']"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문어 용어 행렬 출력\n",
        "print(\"Document term matrix....\")\n",
        "print(chunk_names)\n",
        "for word, item in zip(vocabulary, document_term_matrix.T):\n",
        "  temp = ''.join([f\"{str(i):10s}\" for i in item.data] )\n",
        "  print(f\"{word:10s}{temp}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiLdORC5W-bm",
        "outputId": "648a02ec-18f1-441c-b379-37a2a195a282"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document term matrix....\n",
            "['Chunk-1', 'Chunk-2', 'Chunk-3', 'Chunk-4', 'Chunk-5', 'Chunk-6', 'Chunk-7']\n",
            "and       23        9         9         11        9         17        10        \n",
            "are       2         2         1         1         2         2         1         \n",
            "be        6         8         7         7         6         2         1         \n",
            "by        3         4         4         5         14        3         6         \n",
            "county    6         2         7         3         1         2         2         \n",
            "for       7         13        4         10        7         6         4         \n",
            "in        15        11        15        11        13        14        17        \n",
            "is        2         7         3         4         5         5         2         \n",
            "it        8         6         8         9         3         1         2         \n",
            "of        31        20        20        30        29        35        26        \n",
            "on        4         3         5         10        6         5         2         \n",
            "one       1         3         1         2         2         1         1         \n",
            "said      12        5         7         7         4         3         7         \n",
            "state     3         7         2         6         3         4         1         \n",
            "that      13        8         9         2         7         1         7         \n",
            "the       71        51        43        51        43        52        49        \n",
            "to        11        26        20        26        21        15        11        \n",
            "two       2         1         1         1         1         2         2         \n",
            "was       5         6         7         7         4         7         3         \n",
            "which     7         4         5         4         3         1         1         \n",
            "with      2         2         3         1         2         2         3         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "카테고리 예측기\n",
        "  - 텍스트가 속한 카테고리를 예측하는 데 사용\n",
        "  - 검색엔진은 카테고리 예측기를 사용해서 검색 결과를 관련성에 따라 정렬\n",
        "  -TF-IDF 지표를 사용\n",
        "    - TF(Term Frequency) :문서에서의 단어의 빈도\n",
        "    - IDF(Inverse Document Frequency) : 문서에서 단어가 얼마나 고유한지"
      ],
      "metadata": {
        "id": "D6GKcAPmY_i9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfTransformer"
      ],
      "metadata": {
        "id": "IjccHeRvY_Db"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 카테고리맵 : 5개\n",
        "category_map = {\n",
        "    'talk.politics.misc':'Politics', 'rec.autos' : 'Autos', 'rec.sport.hockey':'Hockey','sci.electronics':'Electronics',\n",
        "    'sci.med':'Medicine'\n",
        "}"
      ],
      "metadata": {
        "id": "23Sx6JeAXsO0"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 확보\n",
        "training_data =  fetch_20newsgroups(subset='train', categories=category_map.keys(), shuffle=True, random_state=5)"
      ],
      "metadata": {
        "id": "1wOT-cPSagQb"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 카운터벡터라이즈를 이용해서 용어개수를 추출\n",
        "count_vectorizer = CountVectorizer()\n",
        "train_tc =  count_vectorizer.fit_transform(training_data.data)\n",
        "train_tc.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5DBCtnibXE3",
        "outputId": "9d0e1c2e-9204-40c9-fb84-d9dd408fbb5f"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2844, 40321)"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF 변형기를 생성하고 데이터를 사용해서 훈련\n",
        "tfidf = TfidfTransformer()\n",
        "train_tfidf = tfidf.fit_transform(train_tc)"
      ],
      "metadata": {
        "id": "JrQs6s-8btjr"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트에 사용할 샘플 입력 문장\n",
        "input_data = [\n",
        "    'You need to be careful with cars when you are driving on slippery roads',\n",
        "    'A lot of devices can be operated wirelessly',\n",
        "    'Players need to be careful when they are close to goal posts',\n",
        "    'Political debates help us understand the perspectives of both sides'\n",
        "]"
      ],
      "metadata": {
        "id": "DcEWyrTFeLlF"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 다항나이브 베이즈 분류기 훈련\n",
        "classifier = MultinomialNB().fit(train_tfidf,training_data.target)"
      ],
      "metadata": {
        "id": "Vs9AIdl8eXFt"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 카운터벡터라이즈.. 입력데이터를 변형\n",
        "input_tc = count_vectorizer.transform(input_data)"
      ],
      "metadata": {
        "id": "oNXaM12delIE"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tfidf 변환기를 사용해서 벡터화된 데이터를 변환\n",
        "input_tfidf = tfidf.transform(input_tc)"
      ],
      "metadata": {
        "id": "RjclLEoZeu99"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 카테고리출력을 예측\n",
        "predictions = classifier.predict(input_tfidf)"
      ],
      "metadata": {
        "id": "5kvr2sGPe5Ti"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결과를 출력\n",
        "for sent,category in zip(input_data, predictions):\n",
        "  print(f\"input:{sent}\\n predicted category : {category_map[training_data.target_names[category]]}\")\n",
        "  print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQvXMrK-fCfV",
        "outputId": "2cb3e4d8-be88-4e2e-d515-babd9975042b"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input:You need to be careful with cars when you are driving on slippery roads\n",
            " predicted category : Autos\n",
            "==================================================\n",
            "input:A lot of devices can be operated wirelessly\n",
            " predicted category : Electronics\n",
            "==================================================\n",
            "input:Players need to be careful when they are close to goal posts\n",
            " predicted category : Hockey\n",
            "==================================================\n",
            "input:Political debates help us understand the perspectives of both sides\n",
            " predicted category : Politics\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ygb4fNg1fhQM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}