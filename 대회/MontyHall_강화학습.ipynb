{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iv0IeCq7jRnq",
        "outputId": "a5863c5a-7d43-46ea-c652-da3a5bfb7dc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습된 Q-Value 테이블:\n",
            "{'0_1': [3.973052686718854, 4.051168521868455], '1_0': [4.820608253620243, 3.6043626048771307], '2_0': [5.611490289119962, 4.722033404701614], '0_2': [9.999999999999979, 8.999999999999979], '1_2': [9.999999999999979, 8.999999999999645], '2_1': [9.999999999999979, 8.999999999999979]}\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class MontyHallAgent:\n",
        "    def __init__(self, epsilon, alpha, gamma, num_doors):\n",
        "        self.epsilon = epsilon  # 탐험 확률\n",
        "        self.alpha = alpha  # 학습 속도\n",
        "        self.gamma = gamma  # 할인 계수\n",
        "        self.num_doors = num_doors  # 문의 개수\n",
        "        self.q_values = {}  # Q-Value 테이블 초기화\n",
        "\n",
        "    def get_state_key(self, player_choice, revealed_door):\n",
        "        return f\"{player_choice}_{revealed_door}\"\n",
        "\n",
        "    def get_action(self, state):\n",
        "        # 탐험 또는 이전에 학습한 Q-Value에 기반하여 행동 선택\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            return random.choice([0, 1])  # 0: 변경하지 않음, 1: 변경\n",
        "        else:\n",
        "            return np.argmax(self.q_values.get(state, [0, 0]))\n",
        "\n",
        "    def update_q_value(self, state, action, reward, next_state):\n",
        "        current_q = self.q_values.get(state, [0, 0])\n",
        "        max_future_q = np.max(self.q_values.get(next_state, [0, 0]))\n",
        "        new_q = current_q[action] + self.alpha * (reward + self.gamma * max_future_q - current_q[action])\n",
        "        current_q[action] = new_q\n",
        "        self.q_values[state] = current_q\n",
        "\n",
        "def play_monty_hall(agent, num_episodes):\n",
        "    for episode in range(num_episodes):\n",
        "        # 초기 상태 설정\n",
        "        player_choice = random.randint(0, 2)\n",
        "        car_door = random.randint(0, 2)\n",
        "        revealed_door = [door for door in range(3) if door != player_choice and door != car_door][0]\n",
        "\n",
        "        # 플레이어 선택 변경 여부 결정\n",
        "        action = agent.get_action(agent.get_state_key(player_choice, revealed_door))\n",
        "\n",
        "        # 보상 계산\n",
        "        reward = 1 if (player_choice == car_door and action == 1) or (player_choice != car_door and action == 0) else 0\n",
        "\n",
        "        # 다음 상태 키 생성\n",
        "        next_state = agent.get_state_key(player_choice, revealed_door)\n",
        "\n",
        "        # Q-Value 업데이트\n",
        "        agent.update_q_value(agent.get_state_key(player_choice, revealed_door), action, reward, next_state)\n",
        "\n",
        "# Monty Hall 에이전트 생성\n",
        "agent = MontyHallAgent(epsilon=0.1, alpha=0.5, gamma=0.9, num_doors=3)\n",
        "\n",
        "# 학습 실행\n",
        "play_monty_hall(agent, num_episodes=10000)\n",
        "\n",
        "# 학습된 Q-Value 테이블 출력\n",
        "print(\"학습된 Q-Value 테이블:\")\n",
        "print(agent.q_values)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L4OWRTSbn0N1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}